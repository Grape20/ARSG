{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ARSG Exercises\n",
        "This notebook is modified from [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), aiming at adding essential features and solving exercises after the tutorial.\n",
        "\n",
        "I have added several features, including:\n",
        " - Train model with longer, more complex and more sentences.\n",
        " - Consider location(or position) information in attention mechanism.\n",
        " - Try with more layers, more hidden units. Compare the training time and results.\n",
        " - Replace the embeddings with pretrained word embeddings such as word2vec or GloVe\n",
        "\n",
        "Features to be constructed include:\n",
        " - Build an automatic evaluation function, and evaluate the model's performance\n",
        " - Try another dataset to build an autoencoder.\n",
        " - Try another dataset to build a conversational model.\n"
      ],
      "metadata": {
        "id": "8u6MNohX4EII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working Logs\n",
        "- Train model with long and complex sentences.  \n",
        "  - By visualization of distribution of dataset I see that actually around 90% of sentences have words less than 10. So instead of increasing the length of sentences in the dataset, I dropped the restriction of specific English prefixes. This brings a ten times of size of training dataset.\n",
        "  - To address the problem of long time trianing on the large dataset, I fully utilized T4 GPU provided by Google Colab by increasing the batch size to $2^{12}$. This accelerates at least 10 times the training process and finish training in around 10min.\n",
        "  - Then I increased the maximum of length of sentences in training dataset to 20. This induced lacking of GPU memory. I reduced batch size to $2^{10}$ to solve this problem. But we may try a better solution. We may use different shape of training tensors to store sentences in different range of length seperately. The training took 40min to complete using T4 GPU provided by Google Colab.\n",
        "  - I implemented the idea above, using different shape of training tensors to store sentences in different range of length seperately, and seperately making dataloaders of different batch size to fully utilize T4 GPU. Then I trained the model, the encoder and the decoder, with all dataloaders. The training took 24min, 40% less than the original training strategy.\n",
        "- Construct an evaluation function, evaluating the model's performance.\n",
        "  - To use a BLEU score, multiple reference texts in dataset is required.\n",
        "- Consider location information in attention mechanism.\n",
        "  - Complete, mainly by modified BahdanauAttention class. I encoded the previous weight in the specific position by multiplying a trainable parameter and add to the sum of query and keys, i.e. $\n",
        "  e_{ij}=a(s_{i-1}, \\alpha_{i-1,j}, h_j)=v_a^T\\tanh(W_as_{i-1}+U_ah_j+P_a\\alpha_{i-1,j}).$\n",
        "- Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
        "  - I constructed APIs for bidirectional model and multilayer model. Specifically, I modified encoder, decoder and BahdanauAttention. In BahdanauAttention I map the multilayer bidirectional query vector and the bidirectional keys vector to fixed size vectors. Then sum them with previous weights.  \n",
        "  - The training time of bidirectional model dramatically increased to 35min. And the results of evaluation reading worse.\n",
        "  - The training time of 2 layer model increase to 32min.\n",
        "  - The training time of model with 2^8 hidden size increase to 31min.\n",
        "- Replace the embeddings with pretrained word embeddings such as word2vec or GloVe\n",
        "  - Reading papers about word2vec.\n",
        "  - I replaced the trainable embedding with word2vec of Gensim. Although Gensim has several pretrained model, I trained Gensim's Word2Vec model is with French corpus in training dataset, and then used it to embed French input.\n",
        "  - Training of ARSG using pretrained word2vec embedding took slightly longer time and training loss is bigger.\n",
        "  \n"
      ],
      "metadata": {
        "id": "KB4T3bxIP_P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Google Colab"
      ],
      "metadata": {
        "id": "ZwG0i9ZUt2i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive and change working dirctory to access dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "if os.getcwd() == '/content':\n",
        "  os.chdir('drive/MyDrive/TorchExperiments/SequencesModel')\n",
        "  print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8buV4mAebsW",
        "outputId": "9d08e3a3-ef0d-41f9-a5ba-7fd89e44dbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/TorchExperiments/SequencesModel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-cok6Fd0TaI"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZE3XQxv0TaL"
      },
      "source": [
        "\n",
        "# NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "**Author**: [Sean Robertson](https://github.com/spro)\n",
        "\n",
        "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
        "write our own classes and functions to preprocess the data to do our NLP\n",
        "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
        "learn how `torchtext` can handle much of this preprocessing for you in the\n",
        "three tutorials immediately following this one.\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence\n",
        "to sequence network](https://arxiv.org/abs/1409.3215)_, in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473)_, which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  :doc:`/beginner/deep_learning_60min_blitz` to get started with PyTorch in general\n",
        "-  :doc:`/beginner/pytorch_with_examples` for a wide and deep overview\n",
        "-  :doc:`/beginner/former_torchies_tutorial` if you are former Lua Torch user\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  [Learning Phrase Representations using RNN Encoder-Decoder for\n",
        "   Statistical Machine Translation](https://arxiv.org/abs/1406.1078)_\n",
        "-  [Sequence to Sequence Learning with Neural\n",
        "   Networks](https://arxiv.org/abs/1409.3215)_\n",
        "-  [Neural Machine Translation by Jointly Learning to Align and\n",
        "   Translate](https://arxiv.org/abs/1409.0473)_\n",
        "-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)_\n",
        "\n",
        "You will also find the previous tutorials on\n",
        ":doc:`/intermediate/char_rnn_classification_tutorial`\n",
        "and :doc:`/intermediate/char_rnn_generation_tutorial`\n",
        "helpful as those concepts are very similar to the Encoder and Decoder\n",
        "models, respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "uhjEbOBEuSYg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4IpFn4C0TaN"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "import gensim.models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0lwPzUt0TaN"
      },
      "source": [
        "## Loading data files\n",
        "\n",
        "The data for this project is a set of many thousands of English to\n",
        "French translation pairs.\n",
        "\n",
        "[This question on Open Data Stack\n",
        "Exchange](https://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages)_\n",
        "pointed me to the open translation site https://tatoeba.org/ which has\n",
        "downloads available at https://tatoeba.org/eng/downloads - and better\n",
        "yet, someone did the extra work of splitting language pairs into\n",
        "individual text files here: https://www.manythings.org/anki/\n",
        "\n",
        "The English to French pairs are too big to include in the repository, so\n",
        "download to ``data/eng-fra.txt`` before continuing. The file is a tab\n",
        "separated list of translation pairs:\n",
        "\n",
        "::\n",
        "\n",
        "    I am cold.    J'ai froid.\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   [here](https://download.pytorch.org/tutorial/data.zip)\n",
        "   and extract it to the current directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8D_JsTL0TaN"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN\n",
        "tutorials, we will be representing each word in a language as a one-hot\n",
        "vector, or giant vector of zeros except for a single one (at the index\n",
        "of the word). Compared to the dozens of characters that might exist in a\n",
        "language, there are many many more words, so the encoding vector is much\n",
        "larger. We will however cheat a bit and trim the data to only use a few\n",
        "thousand words per language.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3UBL0Er0TaO"
      },
      "source": [
        "We'll need a unique index per word to use as the inputs and targets of\n",
        "the networks later. To keep track of all this we will use a helper class\n",
        "called ``Lang`` which has word → index (``word2index``) and index → word\n",
        "(``index2word``) dictionaries, as well as a count of each word\n",
        "``word2count`` which will be used to replace rare words later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaoTxg040TaO"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWHkfM7Y0TaO"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode\n",
        "characters to ASCII, make everything lowercase, and trim most\n",
        "punctuation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IeIzW9f0TaP"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    # s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"([.!?])\", \"\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalizeString(\"J'oublie toujours les noms des gens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8m6a0Y8NMgNh",
        "outputId": "1d3ae035-a2f5-45fa-e4ce-6530652c388e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'j oublie toujours les noms des gens'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qN2_pzO0TaP"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split\n",
        "lines into pairs. The files are all English → Other Language, so if we\n",
        "want to translate from Other Language → English I added the ``reverse``\n",
        "flag to reverse the pairs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lang1, lang2 = 'eng', 'fra'\n",
        "lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "# See the distribution of length of sentences.\n",
        "pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "len_eng = [len(p[0].split(' ')) for p in pairs]\n",
        "from matplotlib import pyplot as plt\n",
        "plt.hist(len_eng, bins=25)"
      ],
      "metadata": {
        "id": "QNFwMDMHzkK1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "6b61263b-b537-46d1-b789-71a2d442c544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([9.5200e+02, 2.4638e+04, 4.7917e+04, 3.5527e+04, 1.6205e+04,\n",
              "        6.5120e+03, 2.4800e+03, 9.8600e+02, 2.3000e+02, 2.0000e+02,\n",
              "        9.4000e+01, 4.1000e+01, 2.4000e+01, 1.8000e+01, 5.0000e+00,\n",
              "        3.0000e+00, 1.0000e+00, 3.0000e+00, 2.0000e+00, 0.0000e+00,\n",
              "        1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 2.0000e+00]),\n",
              " array([ 1.  ,  2.88,  4.76,  6.64,  8.52, 10.4 , 12.28, 14.16, 16.04,\n",
              "        17.92, 19.8 , 21.68, 23.56, 25.44, 27.32, 29.2 , 31.08, 32.96,\n",
              "        34.84, 36.72, 38.6 , 40.48, 42.36, 44.24, 46.12, 48.  ]),\n",
              " <BarContainer object of 25 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_eng = np.array(len_eng)\n",
        "len_eng_15p = len_eng[len_eng > 15]\n",
        "plt.hist(len_eng_15p, bins=25)\n",
        "# plt.hist(len_eng, bins=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "irp-Rj8eCrYb",
        "outputId": "7546c6c4-7de0-4da0-cbcf-c3e51e3bd036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([642., 128.,  72.,  94.,  24.,  17.,  12.,  25.,   5.,   3.,   3.,\n",
              "          2.,   1.,   0.,   4.,   1.,   0.,   1.,   0.,   0.,   0.,   1.,\n",
              "          0.,   0.,   2.]),\n",
              " array([16.  , 17.28, 18.56, 19.84, 21.12, 22.4 , 23.68, 24.96, 26.24,\n",
              "        27.52, 28.8 , 30.08, 31.36, 32.64, 33.92, 35.2 , 36.48, 37.76,\n",
              "        39.04, 40.32, 41.6 , 42.88, 44.16, 45.44, 46.72, 48.  ]),\n",
              " <BarContainer object of 25 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAi+ElEQVR4nO3dfXBU1cHH8V9CXoDAbkgku2RIIAoVUsCXQMNWa62kBIyOlNiKTTW2DIx0Q4VUhHQQFR3D0BeVVkGtA3YKpaVTtMAAxqixlQUxDiOCpkpxEhs2odLsQmxeIPf5o5P7uBCEhQ17knw/M3eG3Hv25twzF/Kdze4SY1mWJQAAAIPERnsCAAAApyNQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABgnLtoTuBAdHR2qr6/X4MGDFRMTE+3pAACA82BZlo4fP6709HTFxn75cyQ9MlDq6+uVkZER7WkAAIALUFdXp+HDh3/pmB4ZKIMHD5b0vwt0OBxRng0AADgfwWBQGRkZ9s/xL9MjA6Xz1zoOh4NAAQCghzmfl2fwIlkAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABgnLtoTMNHIJdsicp5PVhRE5DwAAPQ1PIMCAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOOEHSj/+te/9IMf/ECpqakaMGCAxo8fr3feecc+blmWli1bpmHDhmnAgAHKy8vTRx99FHKOY8eOqaioSA6HQ8nJyZo9e7ZOnDhx8VcDAAB6hbAC5T//+Y+uu+46xcfHa/v27Tp48KB++ctfasiQIfaYlStXatWqVVqzZo327NmjpKQk5efnq6WlxR5TVFSkAwcOqKKiQlu3btWbb76puXPnRu6qAABAjxZjWZZ1voOXLFmit956S3/729+6PG5ZltLT0/XTn/5U999/vyQpEAjI5XJp3bp1mjVrlj744ANlZ2dr7969mjhxoiRpx44duvnmm/Xpp58qPT39nPMIBoNyOp0KBAJyOBznO/3zNnLJtoic55MVBRE5DwAAvUE4P7/Degblr3/9qyZOnKjvfve7SktL0zXXXKPnn3/ePn748GH5/X7l5eXZ+5xOp3Jzc+Xz+SRJPp9PycnJdpxIUl5enmJjY7Vnz54uv29ra6uCwWDIBgAAeq+wAuWf//ynVq9erdGjR2vnzp2aN2+efvKTn+jFF1+UJPn9fkmSy+UKeZzL5bKP+f1+paWlhRyPi4tTSkqKPeZ05eXlcjqd9paRkRHOtAEAQA8TVqB0dHTo2muv1eOPP65rrrlGc+fO1Zw5c7RmzZrump8kqaysTIFAwN7q6uq69fsBAIDoCitQhg0bpuzs7JB9Y8eOVW1trSTJ7XZLkhoaGkLGNDQ02MfcbrcaGxtDjp88eVLHjh2zx5wuMTFRDocjZAMAAL1XWIFy3XXXqaamJmTfP/7xD40YMUKSlJWVJbfbrcrKSvt4MBjUnj175PF4JEkej0dNTU2qrq62x7z22mvq6OhQbm7uBV8IAADoPeLCGbxw4UJ9/etf1+OPP67vfe97evvtt/Xcc8/pueeekyTFxMRowYIFeuyxxzR69GhlZWXpwQcfVHp6umbMmCHpf8+4TJs2zf7VUHt7u0pKSjRr1qzzegcPAADo/cIKlEmTJmnz5s0qKyvT8uXLlZWVpSeffFJFRUX2mAceeEDNzc2aO3eumpqadP3112vHjh3q37+/PWb9+vUqKSnRlClTFBsbq8LCQq1atSpyVwUAAHq0sD4HxRR8DgoAAD1Pt30OCgAAwKVAoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTliB8vDDDysmJiZkGzNmjH28paVFXq9XqampGjRokAoLC9XQ0BByjtraWhUUFGjgwIFKS0vTokWLdPLkychcDQAA6BXiwn3AV7/6Vb366qv/f4K4/z/FwoULtW3bNm3atElOp1MlJSWaOXOm3nrrLUnSqVOnVFBQILfbrV27dunIkSO6++67FR8fr8cffzwClwMAAHqDsAMlLi5Obrf7jP2BQEAvvPCCNmzYoJtuukmStHbtWo0dO1a7d+/W5MmT9corr+jgwYN69dVX5XK5dPXVV+vRRx/V4sWL9fDDDyshIeHirwgAAPR4Yb8G5aOPPlJ6erouv/xyFRUVqba2VpJUXV2t9vZ25eXl2WPHjBmjzMxM+Xw+SZLP59P48ePlcrnsMfn5+QoGgzpw4MBZv2dra6uCwWDIBgAAeq+wAiU3N1fr1q3Tjh07tHr1ah0+fFjf+MY3dPz4cfn9fiUkJCg5OTnkMS6XS36/X5Lk9/tD4qTzeOexsykvL5fT6bS3jIyMcKYNAAB6mLB+xTN9+nT7zxMmTFBubq5GjBihP/3pTxowYEDEJ9eprKxMpaWl9tfBYJBIAQCgF7uotxknJyfrK1/5ij7++GO53W61tbWpqakpZExDQ4P9mhW3233Gu3o6v+7qdS2dEhMT5XA4QjYAANB7XVSgnDhxQocOHdKwYcOUk5Oj+Ph4VVZW2sdrampUW1srj8cjSfJ4PNq/f78aGxvtMRUVFXI4HMrOzr6YqQAAgF4krF/x3H///br11ls1YsQI1dfX66GHHlK/fv105513yul0avbs2SotLVVKSoocDofmz58vj8ejyZMnS5KmTp2q7Oxs3XXXXVq5cqX8fr+WLl0qr9erxMTEbrlAAADQ84QVKJ9++qnuvPNOffbZZxo6dKiuv/567d69W0OHDpUkPfHEE4qNjVVhYaFaW1uVn5+vZ555xn58v379tHXrVs2bN08ej0dJSUkqLi7W8uXLI3tVAACgR4uxLMuK9iTCFQwG5XQ6FQgEuuX1KCOXbIvIeT5ZURCR8wAA0BuE8/Ob/4sHAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYJyLCpQVK1YoJiZGCxYssPe1tLTI6/UqNTVVgwYNUmFhoRoaGkIeV1tbq4KCAg0cOFBpaWlatGiRTp48eTFTAQAAvcgFB8revXv17LPPasKECSH7Fy5cqC1btmjTpk2qqqpSfX29Zs6caR8/deqUCgoK1NbWpl27dunFF1/UunXrtGzZsgu/CgAA0KtcUKCcOHFCRUVFev755zVkyBB7fyAQ0AsvvKBf/epXuummm5STk6O1a9dq165d2r17tyTplVde0cGDB/X73/9eV199taZPn65HH31UTz/9tNra2iJzVQAAoEe7oEDxer0qKChQXl5eyP7q6mq1t7eH7B8zZowyMzPl8/kkST6fT+PHj5fL5bLH5OfnKxgM6sCBA11+v9bWVgWDwZANAAD0XnHhPmDjxo169913tXfv3jOO+f1+JSQkKDk5OWS/y+WS3++3x3wxTjqPdx7rSnl5uR555JFwpwoAAHqosJ5Bqaur03333af169erf//+3TWnM5SVlSkQCNhbXV3dJfveAADg0gsrUKqrq9XY2Khrr71WcXFxiouLU1VVlVatWqW4uDi5XC61tbWpqakp5HENDQ1yu92SJLfbfca7ejq/7hxzusTERDkcjpANAAD0XmEFypQpU7R//37t27fP3iZOnKiioiL7z/Hx8aqsrLQfU1NTo9raWnk8HkmSx+PR/v371djYaI+pqKiQw+FQdnZ2hC4LAAD0ZGG9BmXw4MEaN25cyL6kpCSlpqba+2fPnq3S0lKlpKTI4XBo/vz58ng8mjx5siRp6tSpys7O1l133aWVK1fK7/dr6dKl8nq9SkxMjNBlAQCAnizsF8meyxNPPKHY2FgVFhaqtbVV+fn5euaZZ+zj/fr109atWzVv3jx5PB4lJSWpuLhYy5cvj/RUAABADxVjWZYV7UmEKxgMyul0KhAIdMvrUUYu2RaR83yyoiAi5wEAoDcI5+c3/xcPAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDhhBcrq1as1YcIEORwOORwOeTwebd++3T7e0tIir9er1NRUDRo0SIWFhWpoaAg5R21trQoKCjRw4EClpaVp0aJFOnnyZGSuBgAA9AphBcrw4cO1YsUKVVdX65133tFNN92k2267TQcOHJAkLVy4UFu2bNGmTZtUVVWl+vp6zZw50378qVOnVFBQoLa2Nu3atUsvvvii1q1bp2XLlkX2qgAAQI8WY1mWdTEnSElJ0c9//nPdfvvtGjp0qDZs2KDbb79dkvThhx9q7Nix8vl8mjx5srZv365bbrlF9fX1crlckqQ1a9Zo8eLFOnr0qBISEs7rewaDQTmdTgUCATkcjouZfpdGLtkWkfN8sqIgIucBAKA3COfn9wW/BuXUqVPauHGjmpub5fF4VF1drfb2duXl5dljxowZo8zMTPl8PkmSz+fT+PHj7TiRpPz8fAWDQftZmK60trYqGAyGbAAAoPcKO1D279+vQYMGKTExUffee682b96s7Oxs+f1+JSQkKDk5OWS8y+WS3++XJPn9/pA46TzeeexsysvL5XQ67S0jIyPcaQMAgB4k7EC58sortW/fPu3Zs0fz5s1TcXGxDh482B1zs5WVlSkQCNhbXV1dt34/AAAQXXHhPiAhIUGjRo2SJOXk5Gjv3r166qmndMcdd6itrU1NTU0hz6I0NDTI7XZLktxut95+++2Q83W+y6dzTFcSExOVmJgY7lQBAEAPddGfg9LR0aHW1lbl5OQoPj5elZWV9rGamhrV1tbK4/FIkjwej/bv36/GxkZ7TEVFhRwOh7Kzsy92KgAAoJcI6xmUsrIyTZ8+XZmZmTp+/Lg2bNigN954Qzt37pTT6dTs2bNVWlqqlJQUORwOzZ8/Xx6PR5MnT5YkTZ06VdnZ2brrrru0cuVK+f1+LV26VF6vl2dIAACALaxAaWxs1N13360jR47I6XRqwoQJ2rlzp7797W9Lkp544gnFxsaqsLBQra2tys/P1zPPPGM/vl+/ftq6davmzZsnj8ejpKQkFRcXa/ny5ZG9KgAA0KNd9OegRAOfgwIAQM9zST4HBQAAoLsQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwDoECAACMQ6AAAADjECgAAMA4BAoAADAOgQIAAIxDoAAAAOMQKAAAwDgECgAAMA6BAgAAjEOgAAAA4xAoAADAOAQKAAAwTliBUl5erkmTJmnw4MFKS0vTjBkzVFNTEzKmpaVFXq9XqampGjRokAoLC9XQ0BAypra2VgUFBRo4cKDS0tK0aNEinTx58uKvBgAA9AphBUpVVZW8Xq92796tiooKtbe3a+rUqWpubrbHLFy4UFu2bNGmTZtUVVWl+vp6zZw50z5+6tQpFRQUqK2tTbt27dKLL76odevWadmyZZG7KgAA0KPFWJZlXeiDjx49qrS0NFVVVemGG25QIBDQ0KFDtWHDBt1+++2SpA8//FBjx46Vz+fT5MmTtX37dt1yyy2qr6+Xy+WSJK1Zs0aLFy/W0aNHlZCQcM7vGwwG5XQ6FQgE5HA4LnT6ZzVyybaInOeTFQUROQ8AAL1BOD+/L+o1KIFAQJKUkpIiSaqurlZ7e7vy8vLsMWPGjFFmZqZ8Pp8kyefzafz48XacSFJ+fr6CwaAOHDhwMdMBAAC9RNyFPrCjo0MLFizQddddp3HjxkmS/H6/EhISlJycHDLW5XLJ7/fbY74YJ53HO491pbW1Va2trfbXwWDwQqcNAAB6gAt+BsXr9er999/Xxo0bIzmfLpWXl8vpdNpbRkZGt39PAAAQPRcUKCUlJdq6datef/11DR8+3N7vdrvV1tampqamkPENDQ1yu932mNPf1dP5deeY05WVlSkQCNhbXV3dhUwbAAD0EGEFimVZKikp0ebNm/Xaa68pKysr5HhOTo7i4+NVWVlp76upqVFtba08Ho8kyePxaP/+/WpsbLTHVFRUyOFwKDs7u8vvm5iYKIfDEbIBAIDeK6zXoHi9Xm3YsEEvv/yyBg8ebL9mxOl0asCAAXI6nZo9e7ZKS0uVkpIih8Oh+fPny+PxaPLkyZKkqVOnKjs7W3fddZdWrlwpv9+vpUuXyuv1KjExMfJXCAAAepywAmX16tWSpBtvvDFk/9q1a3XPPfdIkp544gnFxsaqsLBQra2tys/P1zPPPGOP7devn7Zu3ap58+bJ4/EoKSlJxcXFWr58+cVdCQAA6DUu6nNQooXPQQEAoOe5ZJ+DAgAA0B0IFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBx4qI9gd5s5JJtETnPJysKInIeAAB6Cp5BAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHHCDpQ333xTt956q9LT0xUTE6OXXnop5LhlWVq2bJmGDRumAQMGKC8vTx999FHImGPHjqmoqEgOh0PJycmaPXu2Tpw4cVEXAgAAeo+wA6W5uVlXXXWVnn766S6Pr1y5UqtWrdKaNWu0Z88eJSUlKT8/Xy0tLfaYoqIiHThwQBUVFdq6davefPNNzZ0798KvAgAA9Cph/2eB06dP1/Tp07s8ZlmWnnzySS1dulS33XabJOl3v/udXC6XXnrpJc2aNUsffPCBduzYob1792rixImSpF//+te6+eab9Ytf/ELp6ekXcTkAAKA3iOhrUA4fPiy/36+8vDx7n9PpVG5urnw+nyTJ5/MpOTnZjhNJysvLU2xsrPbs2dPleVtbWxUMBkM2AADQe0U0UPx+vyTJ5XKF7He5XPYxv9+vtLS0kONxcXFKSUmxx5yuvLxcTqfT3jIyMiI5bQAAYJge8S6esrIyBQIBe6urq4v2lAAAQDeKaKC43W5JUkNDQ8j+hoYG+5jb7VZjY2PI8ZMnT+rYsWP2mNMlJibK4XCEbAAAoPeKaKBkZWXJ7XarsrLS3hcMBrVnzx55PB5JksfjUVNTk6qrq+0xr732mjo6OpSbmxvJ6QAAgB4q7HfxnDhxQh9//LH99eHDh7Vv3z6lpKQoMzNTCxYs0GOPPabRo0crKytLDz74oNLT0zVjxgxJ0tixYzVt2jTNmTNHa9asUXt7u0pKSjRr1izewdPNRi7ZFpHzfLKiICLnAQDgbMIOlHfeeUff+ta37K9LS0slScXFxVq3bp0eeOABNTc3a+7cuWpqatL111+vHTt2qH///vZj1q9fr5KSEk2ZMkWxsbEqLCzUqlWrInA5AACgN4ixLMuK9iTCFQwG5XQ6FQgEuuX1KJF6piFSIvWMBc+gAACiKZyf3z3iXTwAAKBvIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxgn7o+5x6Zn2ybYAAHQ3nkEBAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGIdAAQAAxiFQAACAcQgUAABgHAIFAAAYh0ABAADGiYv2BNDzjFyyLSLn+WRFQUTOEym99boAoCfiGRQAAGAcAgUAABiHQAEAAMbhNSiIGl7zAQA4G55BAQAAxiFQAACAcfgVD3q8SP2qCABgDp5BAQAAxolqoDz99NMaOXKk+vfvr9zcXL399tvRnA4AADBE1ALlj3/8o0pLS/XQQw/p3Xff1VVXXaX8/Hw1NjZGa0oAAMAQMZZlWdH4xrm5uZo0aZJ+85vfSJI6OjqUkZGh+fPna8mSJV/62GAwKKfTqUAgIIfDEfG58ZoGmIC3T18avN0duHTC+fkdlRfJtrW1qbq6WmVlZfa+2NhY5eXlyefznTG+tbVVra2t9teBQEDS/y60O3S0ft4t5wXCkblwU7Sn0C3efyQ/2lMIEam/79317xEQrnEP7YzIebrj72rn35PzeW4kKoHy73//W6dOnZLL5QrZ73K59OGHH54xvry8XI888sgZ+zMyMrptjgC6h/PJaM+ge/TW60Lf1Z339PHjx+V0Or90TI94m3FZWZlKS0vtrzs6OnTs2DGlpqYqJiamy8cEg0FlZGSorq6uW34N1FOxLl1jXc6Oteka63J2rE3XWJf/PXNy/Phxpaenn3NsVALlsssuU79+/dTQ0BCyv6GhQW63+4zxiYmJSkxMDNmXnJx8Xt/L4XD02Rvhy7AuXWNdzo616RrrcnasTdf6+rqc65mTTlF5F09CQoJycnJUWVlp7+vo6FBlZaU8Hk80pgQAAAwStV/xlJaWqri4WBMnTtTXvvY1Pfnkk2pubtYPf/jDaE0JAAAYImqBcscdd+jo0aNatmyZ/H6/rr76au3YseOMF85eqMTERD300ENn/Gqor2Ndusa6nB1r0zXW5exYm66xLuGJ2uegAAAAnA3/Fw8AADAOgQIAAIxDoAAAAOMQKAAAwDg9OlDefPNN3XrrrUpPT1dMTIxeeumlkOP33HOPYmJiQrZp06ZFZ7KXUHl5uSZNmqTBgwcrLS1NM2bMUE1NTciYlpYWeb1epaamatCgQSosLDzjg/N6o/NZmxtvvPGM++bee++N0owvjdWrV2vChAn2B0h5PB5t377dPt5X7xfp3GvTF++XrqxYsUIxMTFasGCBva8v3zeduloX7pnz06MDpbm5WVdddZWefvrps46ZNm2ajhw5Ym9/+MMfLuEMo6Oqqkper1e7d+9WRUWF2tvbNXXqVDU3N9tjFi5cqC1btmjTpk2qqqpSfX29Zs6cGcVZXxrnszaSNGfOnJD7ZuXKlVGa8aUxfPhwrVixQtXV1XrnnXd000036bbbbtOBAwck9d37RTr32kh973453d69e/Xss89qwoQJIfv78n0jnX1dJO6Z82L1EpKszZs3h+wrLi62brvttqjMxySNjY2WJKuqqsqyLMtqamqy4uPjrU2bNtljPvjgA0uS5fP5ojXNqDh9bSzLsr75zW9a9913X/QmZYghQ4ZYv/3tb7lfutC5NpbF/XL8+HFr9OjRVkVFRcha9PX75mzrYlncM+erRz+Dcj7eeOMNpaWl6corr9S8efP02WefRXtKl1wgEJAkpaSkSJKqq6vV3t6uvLw8e8yYMWOUmZkpn88XlTlGy+lr02n9+vW67LLLNG7cOJWVlenzzz+PxvSi4tSpU9q4caOam5vl8Xi4X77g9LXp1JfvF6/Xq4KCgpD7Q+LfmbOtS6e+fM+crx7xvxlfqGnTpmnmzJnKysrSoUOH9LOf/UzTp0+Xz+dTv379oj29S6Kjo0MLFizQddddp3HjxkmS/H6/EhISzvgPF10ul/x+fxRmGR1drY0kff/739eIESOUnp6u9957T4sXL1ZNTY3+8pe/RHG23W///v3yeDxqaWnRoEGDtHnzZmVnZ2vfvn19/n4529pIffd+kaSNGzfq3Xff1d69e8841pf/nfmydZH69j0Tjl4dKLNmzbL/PH78eE2YMEFXXHGF3njjDU2ZMiWKM7t0vF6v3n//ff3973+P9lSMc7a1mTt3rv3n8ePHa9iwYZoyZYoOHTqkK6644lJP85K58sortW/fPgUCAf35z39WcXGxqqqqoj0tI5xtbbKzs/vs/VJXV6f77rtPFRUV6t+/f7SnY4zzWZe+es+Eq9f/iueLLr/8cl122WX6+OOPoz2VS6KkpERbt27V66+/ruHDh9v73W632tra1NTUFDK+oaFBbrf7Es8yOs62Nl3Jzc2VpF5/3yQkJGjUqFHKyclReXm5rrrqKj311FPcLzr72nSlr9wv1dXVamxs1LXXXqu4uDjFxcWpqqpKq1atUlxcnFwuV5+8b861LqdOnTrjMX3lnglXnwqUTz/9VJ999pmGDRsW7al0K8uyVFJSos2bN+u1115TVlZWyPGcnBzFx8ersrLS3ldTU6Pa2tqQ36v3Rudam67s27dPknr9fXO6jo4Otba29un75Ww616YrfeV+mTJlivbv3699+/bZ28SJE1VUVGT/uS/eN+dal65eXtBX7plw9ehf8Zw4cSKkOA8fPqx9+/YpJSVFKSkpeuSRR1RYWCi3261Dhw7pgQce0KhRo5Sfnx/FWXc/r9erDRs26OWXX9bgwYPt3/c6nU4NGDBATqdTs2fPVmlpqVJSUuRwODR//nx5PB5Nnjw5yrPvXudam0OHDmnDhg26+eablZqaqvfee08LFy7UDTfc0OVbBXuLsrIyTZ8+XZmZmTp+/Lg2bNigN954Qzt37uzT94v05WvTV+8XSRo8eHDIa7ckKSkpSampqfb+vnjfnGtd+vI9E7Zov43oYrz++uuWpDO24uJi6/PPP7emTp1qDR061IqPj7dGjBhhzZkzx/L7/dGedrfrak0kWWvXrrXH/Pe//7V+/OMfW0OGDLEGDhxofec737GOHDkSvUlfIudam9raWuuGG26wUlJSrMTERGvUqFHWokWLrEAgEN2Jd7Mf/ehH1ogRI6yEhARr6NCh1pQpU6xXXnnFPt5X7xfL+vK16av3y9mc/vbZvnzffNEX14V75vzFWJZlXfIqAgAA+BJ96jUoAACgZyBQAACAcQgUAABgHAIFAAAYh0ABAADGIVAAAIBxCBQAAGAcAgUAABiHQAEAAMYhUAAAgHEIFAAAYBwCBQAAGOf/AE3uyf/BiJYYAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_eng = np.array(len_eng)\n",
        "len_eng_25p = len_eng[len_eng > 25]\n",
        "plt.hist(len_eng_25p, bins=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "nGfDRRJPLpNk",
        "outputId": "204dcc67-aac1-467f-d80f-0395ba82ee46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([13.,  5.,  3.,  2.,  1.,  2.,  1.,  0.,  0.,  3.,  1.,  1.,  0.,\n",
              "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  1.]),\n",
              " array([26.  , 26.88, 27.76, 28.64, 29.52, 30.4 , 31.28, 32.16, 33.04,\n",
              "        33.92, 34.8 , 35.68, 36.56, 37.44, 38.32, 39.2 , 40.08, 40.96,\n",
              "        41.84, 42.72, 43.6 , 44.48, 45.36, 46.24, 47.12, 48.  ]),\n",
              " <BarContainer object of 25 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJXeSI3m0TaP"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eufff0880TaQ"
      },
      "source": [
        "Since there are a *lot* of example sentences and we want to train\n",
        "something quickly, we'll trim the data set to only relatively short and\n",
        "simple sentences. Here the maximum length is 10 words (that includes\n",
        "ending punctuation) and we're filtering to sentences that translate to\n",
        "the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced\n",
        "earlier).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "metadata": {
        "id": "DfltswdJJKO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter pairs by max and min length of sentences and English prefixes\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p, max_len, min_len, if_filt_prefix):\n",
        "    return len(p[0].split(' ')) < max_len and \\\n",
        "        len(p[1].split(' ')) < max_len and \\\n",
        "        (p[1].startswith(eng_prefixes) and if_filt_prefix or \\\n",
        "          not if_filt_prefix) and \\\n",
        "        (len(p[0].split(' ')) >= min_len or \\\n",
        "        len(p[1].split(' ')) >= min_len)\n",
        "\n",
        "\n",
        "def filterPairs(pairs, max_len, min_len, if_filt_prefix):\n",
        "    return [pair for pair in pairs if filterPair(pair, max_len, min_len, if_filt_prefix)]"
      ],
      "metadata": {
        "id": "6nr-uAusvaUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NPv-SQ00TaQ"
      },
      "source": [
        "The full process for preparing the data is:\n",
        "\n",
        "-  Read text file and split into lines, split lines into pairs\n",
        "-  Normalize text, filter by length and content\n",
        "-  Make word lists from sentences in pairs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfst4thR0TaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32bb1f63-d23b-4b7b-e08f-196cc1b5a081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17864\n",
            "eng 10698\n",
            "['pourquoi ne pas commander des pizzas ?', 'why don t we order pizza ?']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4O9c5YtUucA",
        "outputId": "bed8236c-3199-46cb-ebda-f16fd79b21ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 135283 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21170\n",
            "eng 12917\n",
            "['notre tour des alpes francaises a velo dura deux semaines', 'our bike tour of the french alps lasted two weeks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data with different length range and filter sequences with English prefixes.\n",
        "def prepareData(lang1, lang2, reverse=False, if_filt_prefix=True, len_split_points=[10]):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    # Count all words in dataset\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "\n",
        "    pairs_list = []\n",
        "    pre_point = 0\n",
        "    for point in len_split_points:\n",
        "      pairs_list.append(filterPairs(pairs, point, pre_point, if_filt_prefix))\n",
        "      print(\"Trimmed to %s sentence pairs of length between %d and %d\" % (len(pairs_list[-1]), pre_point, point))\n",
        "      pre_point = point\n",
        "\n",
        "    return input_lang, output_lang, pairs_list\n",
        "\n"
      ],
      "metadata": {
        "id": "sH2eRZ1IxmXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs_list = prepareData('eng', 'fra', True, True, [10, 20])\n",
        "print(random.choice(pairs_list[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_wvlfbu3XKs",
        "outputId": "ee9d660c-7699-4b47-a4a3-944f7fed9402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135380 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21101\n",
            "eng 12863\n",
            "Trimmed to 11445 sentence pairs of length between 0 and 10\n",
            "Trimmed to 1607 sentence pairs of length between 10 and 20\n",
            "['j accroche au mur une photo de ma grand mere', 'i m hanging a picture of my grandmother on the wall']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = readLangs('eng', 'fra', True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc9osWzDEU9l",
        "outputId": "6c017f25-6fc5-4247-8535-800520b8108d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [pair[0][-1] in '.?!' for pair in pairs]"
      ],
      "metadata": {
        "id": "VQrAfQskGSpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(a), len(a), sep='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQMU4wU6G5F_",
        "outputId": "d33b9fff-1d45-4a5f-ac34-7052d3a1644d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0/135380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = [pair[0][-1] in '.?!' for pair in pairs_list[0]]\n",
        "print(sum(b), len(b), sep='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlLRRZvSHUKd",
        "outputId": "952a634b-a05c-437e-927b-b0ec41cd59b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "353/11445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = [pair[0][-1] in '.?!' for pair in pairs_list[1]]\n",
        "print(sum(c), len(c), sep='/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbcXsCrQHhH6",
        "outputId": "42f2bc67-2ab7-4f1d-af7f-c9ad62078ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103/1607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ND1LWL6vIKcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8fFwu8G0TaQ"
      },
      "source": [
        "## The Seq2Seq Model\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215)_, or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf)_, is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence ``Je ne suis pas le chat noir`` → ``I am not the\n",
        "black cat``. Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. ``chat noir`` and ``black cat``. Because of the ``ne/pas``\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec Model"
      ],
      "metadata": {
        "id": "k3hz9vV_kKBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gensim's word2vec demo"
      ],
      "metadata": {
        "id": "vGFbsmlAutQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('lee_background.cor')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)\n"
      ],
      "metadata": {
        "id": "S2px3y9mkUF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = MyCorpus()\n",
        "model = gensim.models.Word2Vec(sentences=sentences)"
      ],
      "metadata": {
        "id": "lt8Qoi4Vn0pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec_king = model.wv['king']"
      ],
      "metadata": {
        "id": "htqBJvLMn2mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vec_king.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VW1YqO3Ln5Nj",
        "outputId": "53c45595-9873-483d-8022-2bf049b7ffd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gensim's word2vec model trained on dataset"
      ],
      "metadata": {
        "id": "TzPyAmRIu1ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CorpusfromPath:\n",
        "  \"\"\"Corpus of data eng-fra.txt\"\"\"\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "  def __iter__(self):\n",
        "      for line in open(self.path):\n",
        "          # assume there's one document per line, tokens separated by whitespace\n",
        "          yield utils.simple_preprocess(line)"
      ],
      "metadata": {
        "id": "H7zK3itxoa3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = CorpusfromPath(os.path.join('data', 'eng-fra.txt'))\n",
        "vector_size = hidden_size = 2**7\n",
        "workers = 1\n",
        "model = gensim.models.Word2Vec(sentences=corpus, vector_size=vector_size, workers=workers)"
      ],
      "metadata": {
        "id": "CO7Jc4jmpeIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jump_vec = model.wv['jump']"
      ],
      "metadata": {
        "id": "JwoD-zYbrGtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jump_vec.shape)\n",
        "print(jump_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO-SKx9ArZ2a",
        "outputId": "668d0c94-ac91-41ca-ece2-eb964ee62313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128,)\n",
            "[ 0.13227919 -0.01616052  0.06998881  0.2795041  -0.2876691   0.01588585\n",
            "  0.26814267  0.107416    0.2175678  -0.10342063 -0.09447497 -0.37504724\n",
            " -0.16043301 -0.43758395  0.22836854  0.14943987  0.3052172   0.06640745\n",
            "  0.14325139 -0.14543456  0.11801863 -0.25853574 -0.28164187 -0.28306323\n",
            " -0.07489932 -0.1013978  -0.06457721 -0.06121218  0.16933928 -0.12537512\n",
            "  0.324421   -0.12121534 -0.00479255 -0.3626915   0.16110283 -0.06264769\n",
            "  0.09161223 -0.04954673  0.13171832 -0.1854432  -0.12822708  0.26891026\n",
            " -0.3647126   0.17547238  0.07562348 -0.30104446 -0.05813612 -0.10879594\n",
            " -0.3134525   0.15135805 -0.05144145 -0.0367718   0.08154546 -0.06748135\n",
            " -0.08045161  0.00264495 -0.08465866  0.0598894  -0.2952955   0.05942517\n",
            "  0.04634761 -0.06677979  0.19116487  0.10062346  0.03401228 -0.1458146\n",
            " -0.00387106 -0.52553356 -0.18222322 -0.17614947 -0.10937034  0.06801738\n",
            "  0.1677457   0.2167731   0.18734059 -0.17368506  0.11349725 -0.01704138\n",
            " -0.06792911  0.00745813 -0.16824082 -0.03505365  0.07713896  0.11504269\n",
            " -0.05072225  0.16678913  0.13530922  0.06568138  0.3887971   0.24240164\n",
            " -0.22700319  0.02041786 -0.10213336  0.13493642  0.10140886  0.4561601\n",
            " -0.16851035 -0.03070474  0.07894161  0.07229913 -0.03026725 -0.20277293\n",
            "  0.1474434   0.0143968  -0.05742927  0.06792501  0.05786669 -0.10826364\n",
            "  0.0923179  -0.00532964  0.21525761 -0.03384508  0.16027005  0.30303067\n",
            " -0.09461428  0.22693023  0.16709432 -0.07188416 -0.1529603   0.0986905\n",
            "  0.08104298 -0.04140199 -0.10654989  0.11276618  0.00960476  0.03104803\n",
            " -0.07849619 -0.10300988]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gensim's word2vec model trained on filtered sentence pairs list"
      ],
      "metadata": {
        "id": "nAqqn_ua0uxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CorpusfromPairsList:\n",
        "  def __init__(self, pairs_list, tokens=[1]):\n",
        "    self.pairs_list = pairs_list\n",
        "    self.tokens = tokens\n",
        "  def __iter__(self):\n",
        "    for pairs in self.pairs_list:\n",
        "      for pair in pairs:\n",
        "        for take in self.tokens:\n",
        "          yield utils.simple_preprocess(pair[take], min_len=1, max_len=20)"
      ],
      "metadata": {
        "id": "VKhL-DF807ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = CorpusfromPairsList(pairs_list, tokens=[0])\n",
        "vector_size = embedding_size = 2**7\n",
        "workers = 1\n",
        "word2vec = gensim.models.Word2Vec(sentences=corpus, vector_size=vector_size, workers=workers)"
      ],
      "metadata": {
        "id": "f61W6OZ42ebF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = word2vec.wv['merci']\n",
        "print(vec.shape)\n",
        "print(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpvOa0pB2uGo",
        "outputId": "fe5e23a5-26b4-4061-d15b-fbdd68358610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128,)\n",
            "[ 2.00883411e-02 -3.21791880e-02  7.18929842e-02 -6.99628028e-04\n",
            "  2.01362092e-03 -1.71464533e-02  1.57174375e-02 -6.85400842e-03\n",
            "  2.05560066e-02  2.15606391e-03  1.08098544e-01  7.28501985e-03\n",
            " -1.98693816e-02 -5.14457673e-02  7.84033686e-02  1.20887011e-02\n",
            "  1.83637850e-02  8.95479508e-03 -3.15160453e-02 -6.84962841e-03\n",
            "  3.50402519e-02  6.12912290e-02 -2.85655297e-02 -6.20076023e-02\n",
            " -7.36599490e-02  4.49329428e-03 -7.41976574e-02 -9.26210196e-05\n",
            "  2.67393943e-02 -1.36132659e-02 -5.85762709e-02  3.10410745e-02\n",
            " -3.29278479e-03  3.36479843e-02 -1.96991116e-02  3.46627608e-02\n",
            "  1.63713366e-01  3.38017456e-02  6.60938472e-02 -2.45144647e-02\n",
            "  2.29416210e-02  4.79789935e-02  6.61722897e-03 -4.16056700e-02\n",
            "  3.74645293e-02 -1.97785292e-02 -6.51470348e-02 -5.60288951e-02\n",
            "  2.05490384e-02 -3.02178459e-03  6.72079474e-02 -8.67674593e-03\n",
            "  2.56021153e-02  3.09973452e-02  2.88784243e-02  2.27073263e-02\n",
            "  8.84414539e-02  2.65249964e-02 -6.53505474e-02 -1.85769368e-02\n",
            " -6.00793958e-03 -4.90988716e-02  1.53702972e-02 -4.14280482e-02\n",
            "  6.37047514e-02 -3.17146361e-04  2.53386050e-02  1.23132579e-02\n",
            " -1.61655154e-02 -8.77801552e-02  4.90546376e-02  1.06857596e-02\n",
            " -8.76763761e-02 -3.33482847e-02  1.58601869e-02 -3.84474806e-02\n",
            "  2.43064910e-02 -4.45771776e-02 -1.10163596e-02  4.05421257e-02\n",
            " -2.41038762e-02 -6.06087856e-02 -5.16326865e-03  8.72644112e-02\n",
            " -7.61587080e-03  5.56257889e-02  1.04997128e-01 -4.32107188e-02\n",
            "  4.36961427e-02  9.06169489e-02 -4.96855676e-02 -1.55275417e-02\n",
            "  1.56538133e-02 -3.24417762e-02  7.83902332e-02  3.30700465e-02\n",
            " -8.35811421e-02 -5.71720600e-02  1.15006920e-02 -2.02789735e-02\n",
            " -6.41749501e-02 -1.09753087e-02  2.60358080e-02  2.06089625e-03\n",
            "  4.61329892e-02 -4.30610664e-02  5.94500825e-03 -1.77773926e-02\n",
            "  1.26344385e-02 -2.80877165e-02  3.03628035e-02 -1.02768338e-03\n",
            "  1.19125964e-02  2.69554779e-02 -3.71776149e-02  7.97802676e-03\n",
            " -1.22415870e-02  8.06734562e-02  1.39876381e-02  3.22972871e-02\n",
            "  7.83084705e-03 -5.54742031e-02 -6.38938788e-03 -1.21180937e-02\n",
            " -3.09521779e-02 -1.56709906e-02 -1.63871590e-02  2.35244557e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-7-QRw00TaQ"
      },
      "source": [
        "### The Encoder\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCV_rQn60TaR"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multilayer bidirectional GRU encoder.\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "TDK4o_0fj5wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multilayer bidirectional GRU encoder based on word2vec embedding.\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, dropout_p=0.1, num_layers=1, bidirectional=False):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # embedded = self.dropout(self.embedding(input))\n",
        "        embedded = self.dropout(input)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "7M0pgDe2tdjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6uUHo10TaR"
      },
      "source": [
        "### The Decoder\n",
        "\n",
        "The decoder is another RNN that takes the encoder output vector(s) and\n",
        "outputs a sequence of words to create the translation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7mPV6iX0TaR"
      },
      "source": [
        "#### Simple Decoder\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pDRJDqR0TaR"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQuE7GvG0TaR"
      },
      "source": [
        "I encourage you to train and observe the results of this model, but to\n",
        "save space we'll be going straight for the gold and introducing the\n",
        "Attention Mechanism.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lyZm-cl0TaR"
      },
      "source": [
        "#### Attention Decoder\n",
        "\n",
        "If only the context vector is passed between the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        ".. figure:: https://i.imgur.com/1152PYf.png\n",
        "   :alt:\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "Bahdanau attention, also known as additive attention, is a commonly used\n",
        "attention mechanism in sequence-to-sequence models, particularly in neural\n",
        "machine translation tasks. It was introduced by Bahdanau et al. in their\n",
        "paper titled [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)_.\n",
        "This attention mechanism employs a learned alignment model to compute attention\n",
        "scores between the encoder and decoder hidden states. It utilizes a feed-forward\n",
        "neural network to calculate alignment scores.\n",
        "\n",
        "However, there are alternative attention mechanisms available, such as Luong attention,\n",
        "which computes attention scores by taking the dot product between the decoder hidden\n",
        "state and the encoder hidden states. It does not involve the non-linear transformation\n",
        "used in Bahdanau attention.\n",
        "\n",
        "In this tutorial, we will be using Bahdanau attention. However, it would be a valuable\n",
        "exercise to explore modifying the attention mechanism to use Luong attention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.Pa = nn.Linear(1, 1) # encode pre weight in only 1 location j that stores location information\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        # sample, layer, dim; sample, location, dim; broadcastable because layer equals 1 here; 2h^2 weight params, 2h weight params enough however\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        # sample, size1, location\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "        # sample, size1, dim\n",
        "\n",
        "        return context, weights"
      ],
      "metadata": {
        "id": "LWrohC6YY6YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu9Yzm9z0TaR"
      },
      "outputs": [],
      "source": [
        "# Consider location information\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.Pa = nn.Linear(1, 1) # encode pre weight in only 1 location j that stores location information\n",
        "\n",
        "    def forward(self, query, keys, preweights):\n",
        "        # scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        # # sample, layer, dim; sample, location, dim; broadcastable because layer equals 1 here; 2h^2 weight params, 2h weight params enough however\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys) + self.Pa(preweights.permute(0,2,1))))\n",
        "        # sample, layer, dim; sample, location, dim; sample, location, size1;\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        # sample, size1, location\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "        # sample, size1, dim\n",
        "\n",
        "        return context, weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider location information.\n",
        "# Process multilayer hidden state.\n",
        "# Process bi-directional GRU\n",
        "class BahdanauAttention(nn.Module):\n",
        "    # def __init__(self, hidden_size):\n",
        "    def __init__(self, hidden_size, num_layers, bidirectional=False):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        D = 2 if bidirectional else 1\n",
        "        self.Wa = nn.Linear(hidden_size * num_layers * D, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size * D, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.Pa = nn.Linear(1, 1) # encode pre weight in only 1 location j that stores location information\n",
        "\n",
        "\n",
        "    def forward(self, query, keys, pre_weights):\n",
        "        # scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        # # sample, layer, dim; sample, location, dim; broadcastable because layer equals 1 here; 2h^2 weight params, 2h weight params enough however\n",
        "\n",
        "        # scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys) + self.Pa(pre_weights.permute(0,2,1))))\n",
        "        # # sample, layer*D, dim; sample, location, D*dim; sample, location, size1;\n",
        "\n",
        "        scores = self.Va(torch.tanh(self.Wa(torch.flatten(query, 1).unsqueeze(1)) + self.Ua(keys) + self.Pa(pre_weights.permute(0,2,1))))\n",
        "        # sample, 1, layer*D*dim; sample, location, D*dim; sample, location, 1;\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        # sample, size1, location\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "        # sample, size1, dim\n",
        "\n",
        "        return context, weights"
      ],
      "metadata": {
        "id": "ni6oluEwmWco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "skZ1Ck_38RQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt for sequence length\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    # def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_len, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        # for i in range(MAX_LENGTH):\n",
        "        for i in range(max_len):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "4OaPfSds8TTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode pre-weights\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    # def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_len, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        attn_weights = torch.zeros(batch_size, 1, max_len, device=device)\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        # for i in range(MAX_LENGTH):\n",
        "        for i in range(max_len):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                # decoder_input, decoder_hidden, encoder_outputs\n",
        "                decoder_input, decoder_hidden, encoder_outputs, attn_weights\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs, pre_weights):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        # context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs, pre_weights)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "H8b8BXtjUnIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder using Multilayer bidirectional GRU.\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers=1, dropout_p=0.1, bidirectional=False):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size, num_layers, bidirectional=bidirectional)\n",
        "        D = 2 if bidirectional else 1\n",
        "        self.gru = nn.GRU((D+1) * hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        self.out = nn.Linear(hidden_size*D, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    # def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "    def forward(self, encoder_outputs, encoder_hidden, max_len, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        attn_weights = torch.zeros(batch_size, 1, max_len, device=device)\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        # for i in range(MAX_LENGTH):\n",
        "        for i in range(max_len):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                # decoder_input, decoder_hidden, encoder_outputs\n",
        "                decoder_input, decoder_hidden, encoder_outputs, attn_weights\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs, pre_weights):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        # context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs, pre_weights)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        # print(f'input_gru.shape={input_gru.shape}, hidden.shape={hidden.shape}')\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "XomMAw4YkWEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou11_qv50TaS"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in [Effective Approaches to Attention-based Neural Machine\n",
        "  Translation](https://arxiv.org/abs/1508.04025)_.</p></div>\n",
        "\n",
        "## Training\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Training Data\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences."
      ],
      "metadata": {
        "id": "xx6dmlt5vO89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9GogBiW0TaS"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ],
      "metadata": {
        "id": "5FuEqr3m3rkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataloaders for sequences of different length.\n",
        "def get_dataloader_list(batch_size_list, len_split_points, filt_prefix=True):\n",
        "    input_lang, output_lang, pairs_list = prepareData('eng', 'fra', True, filt_prefix, len_split_points)\n",
        "    dataloader_list = [get_dataloader(batch_size_list[idx], pairs_list[idx], len_split_points[idx]) for idx in range(len(len_split_points))]\n",
        "    return input_lang, output_lang, dataloader_list\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size, pairs, max_len):\n",
        "    # input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, max_len), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, max_len), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    # return input_lang, output_lang, train_dataloader\n",
        "    return train_dataloader"
      ],
      "metadata": {
        "id": "s13rqGq63s6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vecs_from_sentence(word2vec_emb, sentence):\n",
        "  return np.array([word2vec_emb.wv[word] for word in sentence.split(' ')])"
      ],
      "metadata": {
        "id": "UJpynT9V_AwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataloaders for sequences of different length.\n",
        "# def get_dataloader_list(batch_size_list, len_split_points, word2vec_emb, filt_prefix=True):\n",
        "def get_dataloader_list(batch_size_list, len_split_points, word2vec_emb, input_lang, output_lang, pairs_list):\n",
        "    # input_lang, output_lang, pairs_list = prepareData('eng', 'fra', True, filt_prefix, len_split_points)\n",
        "    dataloader_list = [get_dataloader(batch_size_list[idx], pairs_list[idx], len_split_points[idx], word2vec_emb) for idx in range(len(len_split_points))]\n",
        "    return dataloader_list\n",
        "\n",
        "# Utilize trained word2vec embedding to embed input.\n",
        "def get_dataloader(batch_size, pairs, max_len, word2vec_emb):\n",
        "    # input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    # input_ids = np.zeros((n, max_len), dtype=np.int32)\n",
        "    input_vecs = np.zeros((n, max_len, word2vec_emb.vector_size))\n",
        "    target_ids = np.zeros((n, max_len), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        # inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        inp_vecs = vecs_from_sentence(word2vec_emb, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        # inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        # input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        input_vecs[idx, :len(inp_vecs), :] = inp_vecs\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    # train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "    #                            torch.LongTensor(target_ids).to(device))\n",
        "    train_data = TensorDataset(torch.Tensor(input_vecs).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    # return input_lang, output_lang, train_dataloader\n",
        "    return train_dataloader"
      ],
      "metadata": {
        "id": "qjHuDtbmv4CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IxTmlb80TaS"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)_.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "        max_len = input_tensor.size(1)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "Ru4htyE9ZhBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3X8u04l0TaS"
      },
      "outputs": [],
      "source": [
        "# Training process per epoch for different sequences of length.\n",
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "        max_len = input_tensor.size(1)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        # decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, max_len, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qu4l8tq0TaS"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU5NzmV20TaS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeRjMRpi0TaS"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Mjs1t10TaS"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwHH-030TaT"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWI6M0Br0TaT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLQPOoyQ0TaT"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hQPcDxP0TaT"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        max_len = input_tensor.size(1)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        # decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, max_len)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate input sentence using word2vec embedding\n",
        "# def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "def evaluate(encoder, decoder, sentence, word2vec_emb, output_lang):\n",
        "    with torch.no_grad():\n",
        "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_tensor = torch.tensor(vecs_from_sentence(word2vec_emb, sentence), device=device).unsqueeze(0)\n",
        "        max_len = input_tensor.size(1)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        # decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden, max_len)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "obSjvPYwVGpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9ZGvbi0TaT"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg10YBCU0TaT"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate input sentence using word2vec embedding\n",
        "def evaluateRandomly(encoder, decoder, pairs, word2vec_emb, output_lang, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], word2vec_emb, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "Vm8QxV92W17k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating\n"
      ],
      "metadata": {
        "id": "E0QavXVUFNAg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jjrE4Ke0TaT"
      },
      "source": [
        "\n",
        "### Training\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2oYe5JE0TaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151c6621-ddfa-452e-dd87-d918b950f32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "0m 37s (- 9m 29s) (5 6%) 1.5377\n",
            "1m 14s (- 8m 38s) (10 12%) 0.6889\n",
            "1m 49s (- 7m 54s) (15 18%) 0.3561\n",
            "2m 24s (- 7m 14s) (20 25%) 0.1976\n",
            "2m 59s (- 6m 34s) (25 31%) 0.1227\n",
            "3m 34s (- 5m 57s) (30 37%) 0.0846\n",
            "4m 9s (- 5m 20s) (35 43%) 0.0648\n",
            "4m 43s (- 4m 43s) (40 50%) 0.0527\n",
            "5m 19s (- 4m 8s) (45 56%) 0.0455\n",
            "5m 53s (- 3m 32s) (50 62%) 0.0410\n",
            "6m 28s (- 2m 56s) (55 68%) 0.0378\n",
            "7m 2s (- 2m 20s) (60 75%) 0.0348\n",
            "7m 37s (- 1m 45s) (65 81%) 0.0331\n",
            "8m 13s (- 1m 10s) (70 87%) 0.0317\n",
            "8m 47s (- 0m 35s) (75 93%) 0.0310\n",
            "9m 22s (- 0m 0s) (80 100%) 0.0293\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase batch size to accelerate.\n",
        "hidden_size = 128\n",
        "batch_size = 256\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOzZsbB0JdfU",
        "outputId": "d71ce110-ed8f-4a63-fc6c-b558363a4fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "0m 4s (- 1m 7s) (5 6%) 2.6081\n",
            "0m 9s (- 1m 4s) (10 12%) 1.5640\n",
            "0m 14s (- 1m 3s) (15 18%) 1.1890\n",
            "0m 19s (- 0m 57s) (20 25%) 0.9213\n",
            "0m 24s (- 0m 52s) (25 31%) 0.7166\n",
            "0m 29s (- 0m 48s) (30 37%) 0.5585\n",
            "0m 33s (- 0m 43s) (35 43%) 0.4366\n",
            "0m 38s (- 0m 38s) (40 50%) 0.3424\n",
            "0m 43s (- 0m 33s) (45 56%) 0.2711\n",
            "0m 47s (- 0m 28s) (50 62%) 0.2168\n",
            "0m 53s (- 0m 24s) (55 68%) 0.1754\n",
            "0m 57s (- 0m 19s) (60 75%) 0.1442\n",
            "1m 2s (- 0m 14s) (65 81%) 0.1205\n",
            "1m 7s (- 0m 9s) (70 87%) 0.1015\n",
            "1m 12s (- 0m 4s) (75 93%) 0.0875\n",
            "1m 16s (- 0m 0s) (80 100%) 0.0755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase batch size to accelerate.\n",
        "hidden_size = 128\n",
        "batch_size = 2**11\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eg-oaYhMCvU",
        "outputId": "cc8bc15a-20ea-455c-df77-97863562230f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 11445 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 4601\n",
            "eng 2991\n",
            "0m 3s (- 0m 52s) (5 6%) 4.8661\n",
            "0m 6s (- 0m 48s) (10 12%) 2.7447\n",
            "0m 10s (- 0m 44s) (15 18%) 2.3956\n",
            "0m 13s (- 0m 40s) (20 25%) 2.2065\n",
            "0m 17s (- 0m 37s) (25 31%) 2.0450\n",
            "0m 20s (- 0m 34s) (30 37%) 1.8948\n",
            "0m 23s (- 0m 30s) (35 43%) 1.7607\n",
            "0m 27s (- 0m 27s) (40 50%) 1.6424\n",
            "0m 30s (- 0m 24s) (45 56%) 1.5382\n",
            "0m 34s (- 0m 20s) (50 62%) 1.4463\n",
            "0m 37s (- 0m 17s) (55 68%) 1.3623\n",
            "0m 41s (- 0m 13s) (60 75%) 1.2836\n",
            "0m 44s (- 0m 10s) (65 81%) 1.2094\n",
            "0m 48s (- 0m 6s) (70 87%) 1.1388\n",
            "0m 51s (- 0m 3s) (75 93%) 1.0715\n",
            "0m 55s (- 0m 0s) (80 100%) 1.0078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase batch size to accelerate and remove the filter of English prefixes.\n",
        "hidden_size = 128\n",
        "batch_size = 2**12\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04ugFMDDNLAe",
        "outputId": "041dc4cd-c404-41ea-f1bb-4e611a2abf51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 17864\n",
            "eng 10698\n",
            "0m 49s (- 12m 15s) (5 6%) 4.1717\n",
            "1m 42s (- 11m 56s) (10 12%) 2.9332\n",
            "2m 32s (- 11m 0s) (15 18%) 2.4736\n",
            "3m 23s (- 10m 11s) (20 25%) 2.1223\n",
            "4m 14s (- 9m 20s) (25 31%) 1.8146\n",
            "5m 7s (- 8m 32s) (30 37%) 1.5595\n",
            "5m 58s (- 7m 40s) (35 43%) 1.3510\n",
            "6m 50s (- 6m 50s) (40 50%) 1.1832\n",
            "7m 40s (- 5m 58s) (45 56%) 1.0486\n",
            "8m 32s (- 5m 7s) (50 62%) 0.9414\n",
            "9m 23s (- 4m 16s) (55 68%) 0.8546\n",
            "10m 15s (- 3m 25s) (60 75%) 0.7828\n",
            "11m 6s (- 2m 33s) (65 81%) 0.7224\n",
            "11m 57s (- 1m 42s) (70 87%) 0.6721\n",
            "12m 49s (- 0m 51s) (75 93%) 0.6276\n",
            "13m 40s (- 0m 0s) (80 100%) 0.5899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the filter of English prefixes and decrease batch size to\n",
        "# adapt for increasing sequence length. 40 min\n",
        "hidden_size = 128\n",
        "batch_size = 2**10\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rcu2H0lWCd7",
        "outputId": "b6bdbcf6-85db-4a8e-87f1-1010342a1a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 135283 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21170\n",
            "eng 12917\n",
            "2m 35s (- 38m 45s) (5 6%) 1.8750\n",
            "5m 9s (- 36m 3s) (10 12%) 1.1693\n",
            "7m 43s (- 33m 26s) (15 18%) 0.8421\n",
            "10m 17s (- 30m 52s) (20 25%) 0.6436\n",
            "12m 51s (- 28m 16s) (25 31%) 0.5271\n",
            "15m 24s (- 25m 40s) (30 37%) 0.4503\n",
            "17m 57s (- 23m 4s) (35 43%) 0.3955\n",
            "20m 30s (- 20m 30s) (40 50%) 0.3551\n",
            "23m 3s (- 17m 55s) (45 56%) 0.3236\n",
            "25m 36s (- 15m 21s) (50 62%) 0.2983\n",
            "28m 9s (- 12m 47s) (55 68%) 0.2774\n",
            "30m 42s (- 10m 14s) (60 75%) 0.2592\n",
            "33m 15s (- 7m 40s) (65 81%) 0.2439\n",
            "35m 48s (- 5m 6s) (70 87%) 0.2304\n",
            "38m 21s (- 2m 33s) (75 93%) 0.2187\n",
            "40m 54s (- 0m 0s) (80 100%) 0.2079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the filter of English prefixes and train with different batch\n",
        "# size adapt for sentences of different length to accelerate. 24min\n",
        "hidden_size = 128\n",
        "batch_size_list = [2**11, 2**10]\n",
        "len_split_points = [10, 20]\n",
        "\n",
        "input_lang, output_lang, train_dataloader_list = get_dataloader_list(batch_size_list, len_split_points, False)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "for train_dataloader in train_dataloader_list:\n",
        "  train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raZLLIeW-GCH",
        "outputId": "6b61b0d3-c244-4dd1-ff5c-998a9a2a6a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21327\n",
            "eng 13035\n",
            "Trimmed to 105692 sentence pairs of length between 0 and 10\n",
            "Trimmed to 29591 sentence pairs of length between 10 and 20\n",
            "0m 56s (- 14m 10s) (5 6%) 3.6314\n",
            "1m 52s (- 13m 6s) (10 12%) 2.4650\n",
            "2m 48s (- 12m 9s) (15 18%) 1.8960\n",
            "3m 44s (- 11m 12s) (20 25%) 1.4977\n",
            "4m 40s (- 10m 16s) (25 31%) 1.2207\n",
            "5m 36s (- 9m 20s) (30 37%) 1.0240\n",
            "6m 32s (- 8m 24s) (35 43%) 0.8819\n",
            "7m 29s (- 7m 29s) (40 50%) 0.7750\n",
            "8m 26s (- 6m 34s) (45 56%) 0.6922\n",
            "9m 22s (- 5m 37s) (50 62%) 0.6265\n",
            "10m 18s (- 4m 41s) (55 68%) 0.5733\n",
            "11m 15s (- 3m 45s) (60 75%) 0.5286\n",
            "12m 11s (- 2m 48s) (65 81%) 0.4915\n",
            "13m 8s (- 1m 52s) (70 87%) 0.4595\n",
            "14m 4s (- 0m 56s) (75 93%) 0.4319\n",
            "15m 2s (- 0m 0s) (80 100%) 0.4074\n",
            "0m 34s (- 8m 35s) (5 6%) 1.2168\n",
            "1m 8s (- 8m 2s) (10 12%) 0.9445\n",
            "1m 43s (- 7m 27s) (15 18%) 0.8103\n",
            "2m 17s (- 6m 53s) (20 25%) 0.7144\n",
            "2m 51s (- 6m 18s) (25 31%) 0.6400\n",
            "3m 26s (- 5m 43s) (30 37%) 0.5778\n",
            "4m 0s (- 5m 9s) (35 43%) 0.5268\n",
            "4m 34s (- 4m 34s) (40 50%) 0.4834\n",
            "5m 9s (- 4m 0s) (45 56%) 0.4474\n",
            "5m 43s (- 3m 25s) (50 62%) 0.4163\n",
            "6m 17s (- 2m 51s) (55 68%) 0.3876\n",
            "6m 51s (- 2m 17s) (60 75%) 0.3640\n",
            "7m 26s (- 1m 43s) (65 81%) 0.3412\n",
            "8m 0s (- 1m 8s) (70 87%) 0.3219\n",
            "8m 34s (- 0m 34s) (75 93%) 0.3048\n",
            "9m 8s (- 0m 0s) (80 100%) 0.2890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An experiment, actually being same as v4. 41min\n",
        "hidden_size = 128\n",
        "batch_size_list = [2**10]\n",
        "len_split_points = [20]\n",
        "\n",
        "input_lang, output_lang, train_dataloader_list = get_dataloader_list(batch_size_list, len_split_points, False)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "for train_dataloader in train_dataloader_list:\n",
        "  train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9kOtOswR-wH",
        "outputId": "f2e95ef1-e182-4cb4-ecee-ccb104a548c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21327\n",
            "eng 13035\n",
            "Trimmed to 135283 sentence pairs of length between 0 and 20\n",
            "2m 35s (- 38m 57s) (5 6%) 1.9052\n",
            "5m 9s (- 36m 5s) (10 12%) 1.3048\n",
            "7m 44s (- 33m 34s) (15 18%) 1.0259\n",
            "10m 19s (- 30m 57s) (20 25%) 0.7788\n",
            "12m 55s (- 28m 25s) (25 31%) 0.6097\n",
            "15m 29s (- 25m 49s) (30 37%) 0.5040\n",
            "18m 5s (- 23m 15s) (35 43%) 0.4326\n",
            "20m 39s (- 20m 39s) (40 50%) 0.3816\n",
            "23m 15s (- 18m 5s) (45 56%) 0.3432\n",
            "25m 50s (- 15m 30s) (50 62%) 0.3126\n",
            "28m 25s (- 12m 55s) (55 68%) 0.2882\n",
            "31m 1s (- 10m 20s) (60 75%) 0.2682\n",
            "33m 35s (- 7m 45s) (65 81%) 0.2511\n",
            "36m 11s (- 5m 10s) (70 87%) 0.2359\n",
            "38m 46s (- 2m 35s) (75 93%) 0.2234\n",
            "41m 22s (- 0m 0s) (80 100%) 0.2119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the filter of English prefixes and train with different batch\n",
        "# size adapt for sentences of different length to accelerate.\n",
        "# Consider position information, that is the pre_weights.\n",
        "hidden_size = 128\n",
        "batch_size_list = [2**12, 2**11]\n",
        "len_split_points = [10, 20]\n",
        "\n",
        "input_lang, output_lang, train_dataloader_list = get_dataloader_list(batch_size_list, len_split_points, False)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "for train_dataloader in train_dataloader_list:\n",
        "  train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Fk2aiddq8s",
        "outputId": "3356bd94-e6d8-49c1-ad86-6da2ad949c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21327\n",
            "eng 13035\n",
            "Trimmed to 105692 sentence pairs of length between 0 and 10\n",
            "Trimmed to 29591 sentence pairs of length between 10 and 20\n",
            "0m 53s (- 13m 25s) (5 6%) 4.2646\n",
            "1m 47s (- 12m 29s) (10 12%) 3.0554\n",
            "2m 40s (- 11m 36s) (15 18%) 2.5987\n",
            "3m 34s (- 10m 42s) (20 25%) 2.2594\n",
            "4m 27s (- 9m 48s) (25 31%) 1.9488\n",
            "5m 21s (- 8m 55s) (30 37%) 1.6824\n",
            "6m 14s (- 8m 1s) (35 43%) 1.4606\n",
            "7m 8s (- 7m 8s) (40 50%) 1.2779\n",
            "8m 2s (- 6m 15s) (45 56%) 1.1300\n",
            "8m 55s (- 5m 21s) (50 62%) 1.0110\n",
            "9m 49s (- 4m 27s) (55 68%) 0.9147\n",
            "10m 43s (- 3m 34s) (60 75%) 0.8347\n",
            "11m 36s (- 2m 40s) (65 81%) 0.7689\n",
            "12m 31s (- 1m 47s) (70 87%) 0.7124\n",
            "13m 26s (- 0m 53s) (75 93%) 0.6640\n",
            "14m 20s (- 0m 0s) (80 100%) 0.6221\n",
            "0m 30s (- 7m 42s) (5 6%) 1.3097\n",
            "1m 1s (- 7m 11s) (10 12%) 1.0844\n",
            "1m 32s (- 6m 40s) (15 18%) 0.9712\n",
            "2m 3s (- 6m 10s) (20 25%) 0.8840\n",
            "2m 34s (- 5m 39s) (25 31%) 0.8137\n",
            "3m 5s (- 5m 8s) (30 37%) 0.7539\n",
            "3m 35s (- 4m 37s) (35 43%) 0.7025\n",
            "4m 6s (- 4m 6s) (40 50%) 0.6574\n",
            "4m 37s (- 3m 35s) (45 56%) 0.6174\n",
            "5m 8s (- 3m 5s) (50 62%) 0.5817\n",
            "5m 39s (- 2m 34s) (55 68%) 0.5494\n",
            "6m 11s (- 2m 3s) (60 75%) 0.5213\n",
            "6m 42s (- 1m 32s) (65 81%) 0.4952\n",
            "7m 13s (- 1m 1s) (70 87%) 0.4708\n",
            "7m 45s (- 0m 31s) (75 93%) 0.4490\n",
            "8m 17s (- 0m 0s) (80 100%) 0.4295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the filter of English prefixes and train with different batch\n",
        "# size adapt for sentences of different length to accelerate.\n",
        "# Consider position information, that is the pre_weights.\n",
        "# Multilayer bidirectional model with more hidden units.\n",
        "hidden_size = 2**8\n",
        "batch_size_list = [2**12, 2**11]\n",
        "len_split_points = [10, 20]\n",
        "filter_prefix = False\n",
        "num_layers = 1\n",
        "bidirectional = False\n",
        "num_epochs = 80\n",
        "\n",
        "input_lang, output_lang, train_dataloader_list = get_dataloader_list(batch_size_list, len_split_points, filter_prefix)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "\n",
        "for train_dataloader in train_dataloader_list:\n",
        "  train(train_dataloader, encoder, decoder, num_epochs, print_every=5, plot_every=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pYY3FfTzGpb",
        "outputId": "20459a1f-c1c0-4fa9-c719-a2f03f62d6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21327\n",
            "eng 13035\n",
            "Trimmed to 105692 sentence pairs of length between 0 and 10\n",
            "Trimmed to 29591 sentence pairs of length between 10 and 20\n",
            "1m 10s (- 17m 43s) (5 6%) 3.5568\n",
            "2m 21s (- 16m 31s) (10 12%) 2.3605\n",
            "3m 31s (- 15m 17s) (15 18%) 1.7714\n",
            "4m 42s (- 14m 7s) (20 25%) 1.3301\n",
            "5m 52s (- 12m 54s) (25 31%) 1.0230\n",
            "7m 2s (- 11m 44s) (30 37%) 0.8152\n",
            "8m 13s (- 10m 33s) (35 43%) 0.6698\n",
            "9m 23s (- 9m 23s) (40 50%) 0.5642\n",
            "10m 33s (- 8m 13s) (45 56%) 0.4853\n",
            "11m 44s (- 7m 2s) (50 62%) 0.4238\n",
            "12m 55s (- 5m 52s) (55 68%) 0.3754\n",
            "14m 6s (- 4m 42s) (60 75%) 0.3363\n",
            "15m 16s (- 3m 31s) (65 81%) 0.3033\n",
            "16m 27s (- 2m 21s) (70 87%) 0.2756\n",
            "17m 38s (- 1m 10s) (75 93%) 0.2516\n",
            "18m 48s (- 0m 0s) (80 100%) 0.2309\n",
            "0m 44s (- 11m 9s) (5 6%) 1.1653\n",
            "2m 14s (- 9m 41s) (15 18%) 0.6353\n",
            "2m 58s (- 8m 56s) (20 25%) 0.5210\n",
            "3m 43s (- 8m 11s) (25 31%) 0.4349\n",
            "4m 28s (- 7m 27s) (30 37%) 0.3705\n",
            "5m 13s (- 6m 42s) (35 43%) 0.3182\n",
            "5m 57s (- 5m 57s) (40 50%) 0.2784\n",
            "6m 42s (- 5m 13s) (45 56%) 0.2441\n",
            "7m 27s (- 4m 28s) (50 62%) 0.2156\n",
            "8m 12s (- 3m 43s) (55 68%) 0.1921\n",
            "8m 57s (- 2m 59s) (60 75%) 0.1708\n",
            "9m 41s (- 2m 14s) (65 81%) 0.1540\n",
            "10m 26s (- 1m 29s) (70 87%) 0.1385\n",
            "11m 11s (- 0m 44s) (75 93%) 0.1248\n",
            "11m 56s (- 0m 0s) (80 100%) 0.1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the filter of English prefixes and train with different batch\n",
        "# size adapt for sentences of different length to accelerate.\n",
        "# Consider position information, that is the pre_weights.\n",
        "# Multilayer bidirectional model with more hidden units.\n",
        "# Experiment word2vec embedding.\n",
        "hidden_size = 2**8\n",
        "batch_size_list = [2**12, 2**11]\n",
        "len_split_points = [10, 20]\n",
        "filter_prefix = False\n",
        "w2v_min_count = 0\n",
        "w2v_workers = 1\n",
        "num_layers = 1\n",
        "bidirectional = False\n",
        "num_epochs = 80"
      ],
      "metadata": {
        "id": "uNPNgW3sEAhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs_list = prepareData('eng', 'fra', True, filter_prefix, len_split_points)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7clY4nfRMB1A",
        "outputId": "40429d42-609b-4c30-ee5d-3589cab45a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 135380 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "fra 21101\n",
            "eng 12862\n",
            "Trimmed to 107809 sentence pairs of length between 0 and 10\n",
            "Trimmed to 27352 sentence pairs of length between 10 and 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = CorpusfromPairsList(pairs_list, tokens=[0])\n",
        "word2vec = gensim.models.Word2Vec(sentences=corpus, min_count=w2v_min_count, vector_size=hidden_size, workers=w2v_workers)\n"
      ],
      "metadata": {
        "id": "mgt7ptQHMD-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_list = get_dataloader_list(batch_size_list, len_split_points, word2vec, input_lang, output_lang, pairs_list)"
      ],
      "metadata": {
        "id": "_6IQcC3IMJPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder = EncoderRNN(input_lang.n_words, hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "encoder = EncoderRNN(hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers=num_layers, bidirectional=bidirectional).to(device)\n"
      ],
      "metadata": {
        "id": "eq_t90d5MLPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_dataloader in train_dataloader_list:\n",
        "  train(train_dataloader, encoder, decoder, num_epochs, print_every=5, plot_every=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19d9G-TJMM8O",
        "outputId": "6fc911a5-44e5-42fc-e893-b23b32acb070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1m 8s (- 17m 13s) (5 6%) 2.1194\n",
            "2m 19s (- 16m 15s) (10 12%) 1.4036\n",
            "3m 29s (- 15m 8s) (15 18%) 1.0666\n",
            "4m 40s (- 14m 1s) (20 25%) 0.8679\n",
            "5m 51s (- 12m 54s) (25 31%) 0.7340\n",
            "7m 3s (- 11m 46s) (30 37%) 0.6372\n",
            "8m 15s (- 10m 36s) (35 43%) 0.5628\n",
            "9m 27s (- 9m 27s) (40 50%) 0.5036\n",
            "10m 38s (- 8m 16s) (45 56%) 0.4555\n",
            "11m 50s (- 7m 6s) (50 62%) 0.4154\n",
            "13m 2s (- 5m 55s) (55 68%) 0.3814\n",
            "14m 14s (- 4m 44s) (60 75%) 0.3525\n",
            "15m 26s (- 3m 33s) (65 81%) 0.3268\n",
            "16m 38s (- 2m 22s) (70 87%) 0.3050\n",
            "17m 50s (- 1m 11s) (75 93%) 0.2854\n",
            "19m 2s (- 0m 0s) (80 100%) 0.2676\n",
            "0m 41s (- 10m 19s) (5 6%) 1.4064\n",
            "1m 22s (- 9m 37s) (10 12%) 0.9727\n",
            "2m 3s (- 8m 56s) (15 18%) 0.7755\n",
            "2m 45s (- 8m 15s) (20 25%) 0.6445\n",
            "3m 26s (- 7m 34s) (25 31%) 0.5498\n",
            "4m 7s (- 6m 52s) (30 37%) 0.4764\n",
            "4m 48s (- 6m 11s) (35 43%) 0.4206\n",
            "5m 30s (- 5m 30s) (40 50%) 0.3742\n",
            "6m 11s (- 4m 48s) (45 56%) 0.3370\n",
            "6m 52s (- 4m 7s) (50 62%) 0.3041\n",
            "7m 33s (- 3m 26s) (55 68%) 0.2748\n",
            "8m 15s (- 2m 45s) (60 75%) 0.2509\n",
            "8m 56s (- 2m 3s) (65 81%) 0.2291\n",
            "9m 37s (- 1m 22s) (70 87%) 0.2100\n",
            "10m 18s (- 0m 41s) (75 93%) 0.1937\n",
            "11m 0s (- 0m 0s) (80 100%) 0.1777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "aoVkuAymFR4O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mgwvZHm0TaX"
      },
      "source": [
        "Set dropout layers to ``eval`` mode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs)"
      ],
      "metadata": {
        "id": "g914zpCrc8sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ0tza1o0TaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6442aeaf-9475-4921-b647-70c3c43f9b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> je ne vais le dire qu une seule fois alors vous feriez mieux d ecouter\n",
            "= i m only going to say this once so you better listen\n",
            "< i m only going to tell the only one time you d better listen <EOS>\n",
            "\n",
            "> je suis vraiment desole pour ce que j ai dit\n",
            "= i am very sorry for what i said\n",
            "< i really am sorry for what i said said <EOS>\n",
            "\n",
            "> je suis aussi fatigue que l on peut l etre\n",
            "= i m as tired as tired can be\n",
            "< i m as tired as the can can be so we\n",
            "\n",
            "> j irai au japon avec ma copine en aout\n",
            "= i m going to japan with my girlfriend in august\n",
            "< i will go to japan with my girlfriend in august\n",
            "\n",
            "> je me rejouis que les choses se soient passees aussi bien\n",
            "= i m glad things went so well\n",
            "< i m glad things went so well as well <EOS>\n",
            "\n",
            "> je vous demande de le faire parce que j ai confiance en vous\n",
            "= i m asking you to do this because i trust you\n",
            "< i m asking you to do this because i trust you <EOS>\n",
            "\n",
            "> je commence a sentir que quelque chose ne va pas\n",
            "= i m beginning to smell a rat\n",
            "< i m feeling something that s wrong <EOS>\n",
            "\n",
            "> je ne suis pas toujours a la maison le dimanche\n",
            "= i am not always at home on sundays\n",
            "< i m not at home on sunday i sunday <EOS>\n",
            "\n",
            "> vous n etes pas autorise a emmener des chiens dans ce batiment\n",
            "= you aren t permitted to bring dogs into this building\n",
            "< you aren t allowed to keep dogs in this building at this building\n",
            "\n",
            "> je suis ravi que vous ayez mis cela sur la table\n",
            "= i m glad you brought that up\n",
            "< i m glad you brought that up on the table <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate model, that trained with sentences of len10 and len20, with sentences of len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model, that trained with sentences of len20, with sentences of len10 and len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIFHB9e3SLNO",
        "outputId": "9408e68a-c84f-4a9f-ec51-0a53d0280bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> je ne suis pas certain de ce qu il pensait\n",
            "= i m not sure what he was thinking\n",
            "< i m not sure what he thought i was thinking <EOS>\n",
            "\n",
            "> elle a pris sa decision et refuse d en etre dissuadee\n",
            "= she s made up her mind and refuses to be talked out of it\n",
            "< she has made her decision and refuses to be used to but\n",
            "\n",
            "> je suis desole d avoir ete dans l incapacite d etre la pour toi\n",
            "= i m sorry that i haven t been able to be here for you\n",
            "< i m sorry to be able to be here for the being you re doing\n",
            "\n",
            "> tu n es pas le seul a ressentir cela\n",
            "= you re not the only one who feels that way\n",
            "< you aren t alone who owns that difficult <EOS>\n",
            "\n",
            "> je suis prete a vous aider si vous voulez que je le fasse\n",
            "= i m willing to help you if you want me to\n",
            "< i m willing to help you if you want me to me to <EOS>\n",
            "\n",
            "> je ne suis pas bon pour faire plusieurs taches en meme temps\n",
            "= i m not good at multitasking\n",
            "< i m not good at multitasking <EOS>\n",
            "\n",
            "> c est un chic type sauf qu il parle trop\n",
            "= he is a nice man except that he talks too much\n",
            "< a good man is wealthy but he speaks too too much\n",
            "\n",
            "> vous etes celle que je voulais rencontrer\n",
            "= you re the one i ve been wanting to meet\n",
            "< you are the one i wanted to meet\n",
            "\n",
            "> il est certain de reussir l examen s il etudie a ce rythme\n",
            "= he is sure to pass the exam if he studies at this rate\n",
            "< it is certain to succeed the exam if he studies in this pace <EOS>\n",
            "\n",
            "> nous commencons a douter de ce que nous pensions savoir\n",
            "= we re starting to question what we thought we knew\n",
            "< we start to cook what we thought we could know better\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model, that consider position information and trained with\n",
        "# sentences of len10 and len20, with sentences of len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReVW4qvpdBw2",
        "outputId": "ce215067-cbc2-4fa9-ff00-0e6df1159f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> il n est pas trop pauvre pour acheter une bicyclette\n",
            "= he is not too poor to buy a bicycle\n",
            "< it s not too poor to buy a bicycle to buy\n",
            "\n",
            "> je suis desole pour la douleur que je vous ai causee a vous et a votre famille\n",
            "= i m sorry for the pain i caused you and your family\n",
            "< i m sorry for the pain i caused you and your family <EOS>\n",
            "\n",
            "> je me fais beaucoup de souci au sujet de ta sante\n",
            "= i m very worried about your health\n",
            "< i worry about a lot of your health <EOS>\n",
            "\n",
            "> je suis desole de t ennuyer mais nous avons un petit probleme\n",
            "= i m sorry to bother you but we ve got a small problem\n",
            "< i m sorry to bother you but we have a small problem <EOS>\n",
            "\n",
            "> nous sommes dans la meme equipe n est ce pas ?\n",
            "= we re on the same team right ?\n",
            "< we re in the same team right ? <EOS>\n",
            "\n",
            "> tu n es pas la seule a avoir ete blessee\n",
            "= you re not the only one that was hurt\n",
            "< you re not the only one who was hurt you <EOS>\n",
            "\n",
            "> elle n est pas chez elle mais a l ecole\n",
            "= she is not home but at school\n",
            "< she is not at home she at school school <EOS>\n",
            "\n",
            "> je suis desole mais pour l instant je suis occupe\n",
            "= i m sorry but i m busy right now\n",
            "< i m sorry but for the moment i m busy busy\n",
            "\n",
            "> elles sont probablement toutes mortes a l heure qu il est\n",
            "= they are probably all dead now\n",
            "< they are probably all dead now than he is <EOS>\n",
            "\n",
            "> ils economisent leur argent pour l acquisition d une maison\n",
            "= they are saving their money for the purchase of a house\n",
            "< they are saving their money for the purchase of an house\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a bidirectional model, that consider position information\n",
        "# and trained with sentences of len10 and len20, with sentences of len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlW8nf4b6Lyy",
        "outputId": "08cc7c62-e019-4b6c-8a13-2804cdd45299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> je vais courir voulez vous venir ?\n",
            "= i m going for a run do you want to come ?\n",
            "< i am going to go out so much\n",
            "\n",
            "> elles ne sont pas tres differentes de qui que ce soit d autre\n",
            "= they are not very different from anybody else\n",
            "< they aren t very different from that much of anybody else <EOS>\n",
            "\n",
            "> tu es assez grand pour t occuper de toi meme\n",
            "= you are old enough to take care of yourself\n",
            "< you are old enough to make up to you taking care\n",
            "\n",
            "> je suis presque sur que je vais avoir besoin d aide\n",
            "= i m pretty sure i m going to need some help\n",
            "< i m pretty sure i m going to need i need some\n",
            "\n",
            "> elle n est pas moins belle que sa s ur\n",
            "= she is no less beautiful than her sister is\n",
            "< she isn t less beautiful than her sister than her sister\n",
            "\n",
            "> je cherche mon portefeuille l avez vous vu ?\n",
            "= i m looking for my wallet have you seen it ?\n",
            "< i m looking for my wallet for my wallet have\n",
            "\n",
            "> je cherche l emplacement ideal pour accrocher cette photo\n",
            "= i m looking for the perfect spot to hang this picture\n",
            "< i m looking for the perfect spot that it has\n",
            "\n",
            "> je vais dans ma chambre pour pouvoir etudier\n",
            "= i m going to my room so i can study\n",
            "< i m going to go to my room so\n",
            "\n",
            "> je ne suis plus jeune mais je peux encore mordre\n",
            "= i am no longer young but i can still bite\n",
            "< i m not young but i can still be still bite\n",
            "\n",
            "> ils ne sont pas plus semblables qu une vache a un canari\n",
            "= they are no more alike than a cow and a canary\n",
            "< they aren t much bigger than a cow to a cow <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a 2 layer model, that consider position information\n",
        "# and trained with sentences of len10 and len20, with sentences of len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN7iGiboDjRx",
        "outputId": "19a2d0d4-542d-4460-ef73-17fed41eddb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> tu ne vas jamais croire ce qui est arrive aujourd hui\n",
            "= you re never going to believe what happened today\n",
            "< you ll never believe what happened to today today <EOS>\n",
            "\n",
            "> je suis plutot sure que je n ai pas dit ca\n",
            "= i m pretty sure i didn t say that\n",
            "< i m pretty sure i didn t tell that <EOS>\n",
            "\n",
            "> je recois une allocation mensuelle de cinquante mille yens\n",
            "= i am given a monthly allowance of fifty thousand yen\n",
            "< i m given a call of making yen yen yen\n",
            "\n",
            "> je suis desolee que les choses n aient pas fonctionne\n",
            "= i m sorry things didn t work out\n",
            "< i m sorry things didn t work until <EOS>\n",
            "\n",
            "> vous etes les etudiants de tom n est ce pas ?\n",
            "= you re tom s students right ?\n",
            "< you re tom s students of tom aren t you ? <EOS>\n",
            "\n",
            "> je suis content que tu n aies pas appele tom\n",
            "= i m glad you didn t call tom\n",
            "< i m glad you didn t call tom tom <EOS>\n",
            "\n",
            "> nous n avons pas de chance une fois de plus\n",
            "= we re out of luck again\n",
            "< we don t have a chance of more than it is\n",
            "\n",
            "> j ai peur qu elle n accepte pas mon explication\n",
            "= i m afraid she won t accept my explanation\n",
            "< i m afraid she didn t call my explanation <EOS>\n",
            "\n",
            "> je vais rester la bas pendant environ une semaine\n",
            "= i m going to stay there for about a week\n",
            "< i m going to stay there for a week about\n",
            "\n",
            "> nous n avons plus de papier de soie je dois donc aller en acheter\n",
            "= we re out of tissue paper so i need to go buy some\n",
            "< we have out of tissue vegetables i needed to go to buy anything <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a 2^8 hidden size model, that consider position information\n",
        "# and trained with sentences of len10 and len20, with sentences of len20.\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnjgm30kNMZY",
        "outputId": "cf9db108-42a0-4730-d64e-998ed2d9f456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> je suis sur que tom a fait ce qu il a dit\n",
            "= i m pretty sure tom did what he said he did\n",
            "< i m sure tom did tom do what he said he said <EOS>\n",
            "\n",
            "> je cherche l emplacement ideal pour accrocher cette photo\n",
            "= i m looking for the perfect spot to hang this picture\n",
            "< i m looking for the perfect spot to hang this\n",
            "\n",
            "> il est extremement pessimiste et n a pas d ambitions\n",
            "= he is extremely pessimistic and has no aspirations\n",
            "< he is extremely pessimistic and has no aspirations didn t have\n",
            "\n",
            "> tu es la derniere personne que je m attendais a voir ici\n",
            "= you are the last person i would have expected to see here\n",
            "< you re the last person i expected to see here here <EOS>\n",
            "\n",
            "> tu es la femme la plus belle au monde\n",
            "= you re the most beautiful woman in the whole world\n",
            "< you re the most beautiful woman in the whole world\n",
            "\n",
            "> nous sommes occupes a preparer notre mariage et notre lune de miel\n",
            "= we are busy preparing for our wedding and honeymoon\n",
            "< we are busy preparing for our wedding and honeymoon <EOS>\n",
            "\n",
            "> elle est occupee a l heure qu il est et ne peut pas vous parler\n",
            "= she s busy now and can t speak to you\n",
            "< she s busy now and can t speak to you <EOS>\n",
            "\n",
            "> il se plaint tout le temps d une chose ou l autre\n",
            "= he is complaining about something or other all the time\n",
            "< he complains of any time or anything else to the other time <EOS>\n",
            "\n",
            "> je suis sure que vous ne m avez jamais dit de ne pas faire ceci\n",
            "= i m sure you never told me not to do this\n",
            "< i m sure you never told me not to do this <EOS>\n",
            "\n",
            "> je suis presque sure que c est le but de tom\n",
            "= i m pretty sure that s tom s goal\n",
            "< i m pretty sure that s tom s goal <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a 2^8 hidden size model, that consider position information\n",
        "# and trained with sentences of len10 and len20, with sentences of len20.\n",
        "# Evaluate word2vec embedding\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder, pairs_list[1], word2vec, output_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KN8eEYRWVi2",
        "outputId": "7b42152f-1313-4ae4-b251-0528f754931a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> je regarde la television tout au long de la journee\n",
            "= i watch television all day long\n",
            "< i watched television all day long i m working all\n",
            "\n",
            "> j attends toujours mon petit dejeuner veuillez me l apporter maintenant\n",
            "= i m still waiting for my breakfast bring it to me now please\n",
            "< i m still about my breakfast i ll help him now\n",
            "\n",
            "> je n avais pas l intention d interrompre ta conversation\n",
            "= i didn t mean to interrupt your conversation\n",
            "< i didn t mean to interrupt your conversation <EOS>\n",
            "\n",
            "> il essaya de se conduire le plus bravement possible tandis qu il etait pris en otage\n",
            "= he tried to behave as bravely as possible while he was being held hostage\n",
            "< he tried to behave as bravely as possible while he was being held hostage <EOS>\n",
            "\n",
            "> ce n est pas tache facile de le suivre\n",
            "= it s no easy task to keep up with him\n",
            "< it s easy easy to keep it up smoking\n",
            "\n",
            "> j ai le sentiment que vous serez un tres bon avocat\n",
            "= i have a feeling you ll be a very good lawyer\n",
            "< i have the feeling you ll be a very good lawyer\n",
            "\n",
            "> on peut faire tout ce qu on veut selon ce qui nous chante\n",
            "= you can do anything you want depending on what floats your boat\n",
            "< you can do anything you want depending on what floats your boat <EOS>\n",
            "\n",
            "> pour autant que je sache c est l unique traduction\n",
            "= to the best of my knowledge this is the only translation available\n",
            "< as far as i know this is the perfect translation\n",
            "\n",
            "> son histoire etait trop ridicule pour que quiconque y croie\n",
            "= his story was too ridiculous for anyone to believe\n",
            "< his story was too ridiculous for anyone to believe it\n",
            "\n",
            "> l ami de tout le monde n est l ami de personne\n",
            "= a friend to everybody is a friend to nobody\n",
            "< everyone s friend s friend s friend <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-bv3Kdh0TaX"
      },
      "source": [
        "### Visualizing Attention\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix. For a better viewing experience we will do the\n",
        "extra work of adding axes and labels:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3JoJbD50TaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5042fa3-5d09-4a12-e03b-33e0c31dab33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = il n est pas aussi grand que son pere\n",
            "output = he is not as tall as his father <EOS>\n",
            "input = je suis trop fatigue pour conduire\n",
            "output = i am too tired to drive <EOS>\n",
            "input = je suis desole si c est une question idiote\n",
            "output = i m sorry if this is a stupid question <EOS>\n",
            "input = je suis reellement fiere de vous\n",
            "output = i m really proud of you <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-332b9c61fc98>:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "<ipython-input-22-332b9c61fc98>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('il n est pas aussi grand que son pere')\n",
        "\n",
        "evaluateAndShowAttention('je suis trop fatigue pour conduire')\n",
        "\n",
        "evaluateAndShowAttention('je suis desole si c est une question idiote')\n",
        "\n",
        "evaluateAndShowAttention('je suis reellement fiere de vous')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojtxnn1N0TaX"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "-  Try with a different dataset\n",
        "\n",
        "   -  Another language pair\n",
        "   -  Human → Machine (e.g. IOT commands)\n",
        "   -  Chat → Response\n",
        "   -  Question → Answer\n",
        "\n",
        "-  Replace the embeddings with pretrained word embeddings such as ``word2vec`` or\n",
        "   ``GloVe``\n",
        "-  Try with more layers, more hidden units, and more sentences. Compare\n",
        "   the training time and results.\n",
        "-  If you use a translation file where pairs have two of the same phrase\n",
        "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
        "   this:\n",
        "\n",
        "   -  Train as an autoencoder\n",
        "   -  Save only the Encoder network\n",
        "   -  Train a new Decoder for translation from there\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}